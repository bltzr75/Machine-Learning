# -*- coding: utf-8 -*-
"""BankLoanNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MzCQNH8itG3TqWbN36GRWM9iDgwmJYbW
"""

from sklearn import svm
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE
from sklearn.preprocessing import MinMaxScaler
import warnings
from collections import Counter
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from keras import Sequential
from keras.layers import Dense
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model                 #Loaded after getting errors from Google Colab
from tensorflow.keras.layers import Dense, Dropout, Flatten,Input     #Loaded after getting errors from Google Colab



from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['bank_loan.csv']))
df

df = df.dropna()                       # Drop missing Values
print(str(df.isna().any()))            # Check for Missing Values
df.drop('Loan_ID', axis=1)             # Drop unnecesariy ID column
df['LoanAmount'] = (df['LoanAmount'] * 1000).astype(int)  # Scale  Loan Amount 1000 times
Counter(df['Loan_Status'])             # Check for Imbalance in our Dependant Variable (Loan_Status)

pred_y = df['Loan_Status']               # Prepare our Prediction Y extracting the Dependant Variables
pred_X = df.drop('Loan_Status', axis=1)  # Dropping the Dependant Variable from our Independants Variables
                                         # For getting rid of categorical data we can:  
print(type(pred_y))                      # Use df_example.map(dict(Y=1,N = 0)) on Series types (Series are one-dimensional objects that can hold any data type)
print(type(pred_X))                      # Use np.get_dummies() on DataFrame types (A DataFrame is a two dimensional object that can have columns with potential different types)
dm_y = pred_y.map(dict(Y=1, N=0))        # Transforming Categorical Data into 1's and 0's in our Dependants Vars by mapping dicts
dm_X = pd.get_dummies(pred_X)            # Transforming Categorical Data into 1's and 0's in our Independants Vars w/ get_dummies()
print(dm_y)                              # Checking our Dummy Variables
print(dm_X)                              # Checking our Dummy Variables

smote = SMOTE(ratio='minority')           # preparing our variable with the SMOTE for Over Sampling over the imbalanced values of dm_y 
X1, y = smote.fit_sample(dm_X, dm_y)      # Resample our Datasets isun the SMOTE 
print(str(Counter(y)))                    # Check for Imbalances in our Dependant Variable (Loan_Status)
scale= MinMaxScaler()                     # Scale bettween the Minimal Value and the Maximal Value in the columns
X = scale.fit_transform(X1)               # Scaling removing the mean and scaling to unit variance

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42, shuffle=True) # Split data

classifier = Sequential() # Prepare our Classifier Model for adding Hidden Layers with Sequential class
classifier.add(Dense(200, activation='relu', kernel_initializer='random_normal', input_dim=X_test.shape[1])) # First Hidden Layer 
classifier.add(Dense(400, activation='relu', kernel_initializer='random_normal'))                            # Second Hidden Layer
classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))                              # Third Hidden Layer
classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))                           # ONE Output: Approved or Rejected 
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy'])                      # Compile Layers looking for Truely Responses (Accuracy)
classifier.fit(X_train, y_train, batch_size=20, epochs= 50, verbose=0)                                       # Fit our Datasets into the Classifier Model
eval_model = classifier.evaluate(X_train,y_train)                                                            # Returns the loss value & Accuracy values
eval_model

y_pred = classifier.predict(X_test)     # Use our Model to Predict/Classify
y_pred = (y_pred>0.52)                  # Raise the Threshold to 0.52

from sklearn.metrics import confusion_matrix      
cm = confusion_matrix(y_test, y_pred)             # Make a Confussion Matrix 
ax = plt.subplot()                                # Graph the CM 
sns.heatmap(cm, annot=True, ax= ax,);             # Get the Inner Annotations and the Heat Map using SeaBorn

#Labels, title and ticks
ax.set_xlabel('Predicted');ax.set_ylabel('Actual');       #Labels for the Graph
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['no', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes']);

import pickle                               # Use Pickle to export our Model
from sklearn.externals import joblib        
filename = 'loan_model.pkl'
joblib.dump(classifier, filename)

from sklearn.model_selection import StratifiedKFold                           
kfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 0) 
csvscores  =[]
for train, test, in kfold.split(X,y):
  # Creates a model
  model = Sequential()
  model.add(Dense(200, activation='relu'))
  model.add(Dense(400, activation='relu'))
  model.add(Dense(4, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  # Compile model
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy'])
  # Fit the model
  model.fit(X[train], y[train], epochs= 100, verbose=0)
  # Evaluate model
  scores = model.evaluate(X[test], y[test], verbose=0)
  print('%s: %.2f%%' % (model.metrics_names[1], scores[1]*100))
  csvscores.append(scores[1] * 100 )
print('%.2f%% (+/- %.2f%%)' % (np.mean(csvscores), np.std(csvscores)) )